groups:
- name: aptos
  rules:
  - alert: AptosNodeDown
    expr: avg by(instance) (increase(aptos_state_sync_version{chain="mainnet", type="synced"}[1m])) < 1
    for: 1m
    labels:
      severity: critical
      job: aptos
    annotations:
      summary: "Aptos Node Down"
      description: "Service {{ $labels.job }} {{ $labels.chain }} is not syncing."

- name: Node/server alerts
  rules:
  - alert: InstanceDown
    # Condition for alerting
    expr: up == 0
    for: 3m
    # Annotation - additional informational labels to store more information
    annotations:
      title: 'Instance {{ $labels.instance }} down'
      description: '{{ $labels.job }} on {{ $labels.instance }} has been down for more than 3 minutes'
    # Labels - additional labels to be attached to the alert
    labels:
      severity: 'critical'

  - alert: NodeFilesystemReadonly
    expr: node_filesystem_readonly{fstype!~"rootfs|nfs4"} > 0
    for: 5m
    labels:
      severity: critical
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" is read-only.'

  - alert: NodeDiskFull48H
    expr: predict_linear(node_filesystem_free_bytes{fstype!~"rootfs|nfs4|tmpfs"}[4h], 48 * 3600) < 0
    for: 5m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" will be out of diskspace within 48 hours.'

  - alert: NodeDiskFull1H
    expr: predict_linear(node_filesystem_free_bytes{fstype!~"rootfs|nfs4|tmpfs"}[4h], 3600) < 0
    for: 5m
    labels:
      severity: critical
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" will be out of diskspace within 1 hour.'

  - alert: NodeDiskFull
    expr: node_filesystem_avail_bytes/node_filesystem_size_bytes < 0.01
    for: 5m
    labels:
      severity: critical
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" is out of diskspace (< 1% free).'

  - alert: NodeInodeFull48H
    expr: predict_linear(node_filesystem_files_free{fstype!~"rootfs|nfs4|tmpfs"}[4h], 48 * 3600) < 0
    for: 5m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" will be out of inode numbers within 48 hours.'

  - alert: NodeInodeFull1H
    expr: predict_linear(node_filesystem_files_free{fstype!~"rootfs|nfs4|tmpfs"}[4h], 3600) < 0
    for: 5m
    labels:
      severity: critical
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" will be out of inode numbers within 1 hour.'

  - alert: NodeInodeFull
    expr: node_filesystem_files_free/node_filesystem_files < 0.01
    for: 5m
    labels:
      severity: critical
      service: node_exporter
    annotations:
      description: 'Filesystem "{{ $labels.mountpoint }}" on "{{ $labels.instance }}" out of inodes (< 1% free).'

  - alert: NodeOutOfMemory
    expr: node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes * 100 < 10
    for: 2m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Node memory is filling up < 10% left\n  VALUE = {{ $value }}\n LABELS: {{ $labels.instance }}'

  - alert: NodeHighCPULoad
    expr: 100 - (avg by(instance) (rate(node_cpu_seconds_total{mode="idle"}[2m])) * 100) > 80
    for: 0m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'CPU load is > 80%\n  VALUE = {{ $value }}\n LABELS: {{ $labels.instance }}'

  - alert: NodeTimeOutOfSync
    expr: node_timex_sync_status{} != 1
    for: 5m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Time on instance "{{ $labels.instance }}" not in sync with NTP.'

  - alert: NodeTextfileCollectorDown
    expr: time() - node_textfile_mtime_seconds{} > 3000
    for: 5m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Node-exporter textfile collector for file "{{ $labels.file }}" on "{{ $labels.instance }}" has been down for 5 minutes.'

  - alert: NodeTextfileScrapingError
    expr: node_textfile_scrape_error != 0
    for: 5m
    labels:
      severity: major
      service: node_exporter
    annotations:
      description: 'Node-exporter textfile collector scraping error on "{{ $labels.instance }}".'

- name: Cosmos Monitoring
  rules:
  - alert: TooFewPeers
    expr: tendermint_p2p_peers{job="cosmos"} < 2
    for: 5m
    labels:
      severity: major
      service: cosmos-monitoring
    annotations:
      description: 'P2P Peers on `{{ $labels.instance }}` is lower than threshold (current value: {{ $value }})'

  - alert: MissingBlocks
    expr: increase(cosmos_validator_missed_blocks[5m]) > 10
    for: 5m
    labels:
      severity: major
      service: cosmos-monitoring
    annotations:
      description: 'Validator `{{ $labels.moniker }}` is missing `{{ $value }}` blocks!'

  - alert: DegradedSyncing
    expr: increase(tendermint_consensus_latest_block_height[5m]) < 10
    for: 5m
    labels:
      severity: major
      service: cosmos-monitoring
    annotations:
      description: 'Degraded syncing performance - Job {{ $labels.job }} on {{ $labels.instance }}'

  - alert: LowInRank
    expr: cosmos_validator_rank > (count(cosmos_validators_active == 1) - 10)
    for: 5m
    labels:
      severity: major
      service: cosmos-monitoring
    annotations:
      description: 'Your validator `{{ $labels.moniker }}` rank is `{{ $value }}`!'
      
  - alert: IsJailed
    expr: cosmos_validator_jailed == 1
    for: 5m
    labels:
      severity: critical
      service: cosmos-monitoring
    annotations:
      description: 'Your validator `{{ $labels.moniker }}` is jailed! `{{ $value }}`!'

- name: Near Monitoring
  rules:
    - alert: InstanceDown
      expr: up == 0
      for: 1m
      labels:
        severity: "critical"
      annotations:
        summary: "Endpoint {{ $labels.instance }} down"
        description: "{{ $labels.instance }} of job {{ $labels.job }} "
    - alert: NearVersionBuildNotMatched
      expr: near_version_build{instance="yournode.io", job="near"} != near_dev_version_build{instance="yournode.io", job="near"}
      for: 5m
      labels:
        severity: critical
        labels: near
      annotations:
        summary: "Near Node Version needs updated."
        description: "Your version is out of date and you risk getting kicked."
    - alert: StakeBelowSeatPrice
      expr: abs((near_current_stake / near_seat_price) * 100) < 100
      for: 2m
      labels:
        severity: critical
        labels: near
      annotations:
        description: 'Pool is below the current seat price'
    - alert: NearBlockNumberNotIncreasing
      expr: increase(near_block_number[5m]) <= 0
      for: 5m
      labels:
        severity: critical
        labels: near
      annotations:
        summary: "Near Block Number Not Increasing"
        description: "The near_block_number has not increased in the last 5 minutes."
    - alert: NearChunkProducedNotIncreasing
      expr: increase(near_epoch_chunks_produced_number[5m]) <= 0 and near_epoch_chunks_produced_number < near_epoch_chunks_expected_number
      for: 30m
      labels:
        severity: critical
        labels: near
      annotations:
        summary: "Near Chunks Produced Not Increasing"
        description: "The near_epoch_chunks_produced_number has not increased in the last 5 minutes."

- name: peggo_alerts
  rules:
  - alert: HighNonceDifference
    expr: |
      abs(peggo_network_nonce - peggo_orchestrator_nonce) > 5 
      and 
      changes(peggo_orchestrator_nonce[15m]) <= 0
    for: 15m
    labels:
      severity: critical
      job: peggo
    annotations:
      summary: "High difference between peggo_orchestrator_nonce and peggo_network_nonce"
      description: "The difference between peggo_orchestrator_nonce and peggo_network_nonce has been greater than 5 for more than 15 minutes."

  - alert: LowOrchestratorBalance
    expr: peggo_orchestrator_balance <= 0.05
    for: 15m
    labels:
      severity: critical
      job: peggo
    annotations:
      summary: "Orchestrator balance is low"
      description: "Refill orchestrator wallet."

  - alert: PeggoServiceDown
    expr: node_systemd_unit_state{name="peggo.service", state="active"} == 0
    for: 15m
    labels:
      severity: critical
      job: peggo
    annotations:
      summary: "Peggo is down"
      description: "The systemd service 'peggo.service' has been down for more than 15 minutes."


- name: sei price feeder alerts
  rules:
  - record: sei_oracle_vote_penalty_rate
    expr: (sum by(validator) (sei_oracle_vote_penalty_count{chain="pacific-1", type=~"abstain|miss", validator="seivaloper140l6y2gp3gxvay6qtn70re7z2s0gn57zl6nups"}) / sum by(validator) (sei_oracle_vote_penalty_count{chain="pacific-1", type=~"abstain|miss|success", validator="seivaloper140l6y2gp3gxvay6qtn70re7z2s0gn57zl6nups"}))

  - alert: SeiPriceFeeder20PercentMissed
    expr: sei_oracle_vote_penalty_rate > 0.25
    for: 30m
    labels:
      severity: warning
      job: pricefeeder
    annotations:
      summary: "High penalty rate on validator"
      description: "Validator {{ $labels.validator }} has a penalty rate greater than 25 percent."

- name: aleo
  rules:
  - alert: AleoLatestHeightNotIncreasing
    expr: increase(aleo_latest_height[5m]) <= 0
    for: 5m
    labels:
      severity: critical
    annotations:
      summary: "Aleo latest height has not increased in the last 5 minutes"

- name: targets
  rules:
  - alert: monitor_service_down
    expr: up == 0
    for: 30s
    labels:
      severity: critical
    annotations:
      summary: "Monitor service non-operational"
      description: "Service {{ $labels.instance }} is down."
